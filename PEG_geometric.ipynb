{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1dce0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        \n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "import math\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "import torch\n",
    "from torch import nn, einsum, broadcast_tensors\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, matmul, fill_diag, sum as sparsesum, mul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.typing import Adj, Size, OptTensor, Tensor\n",
    "\n",
    "class CoorsNorm(nn.Module):\n",
    "    def __init__(self, eps = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.fn = nn.Sequential(nn.LayerNorm(1), nn.GELU())\n",
    "\n",
    "    def forward(self, coors):\n",
    "        norm = coors.norm(dim = -1, keepdim = True)\n",
    "        normed_coors = coors / norm.clamp(min = self.eps)\n",
    "        phase = self.fn(norm)\n",
    "        return (phase * normed_coors)\n",
    "\n",
    "    \n",
    "class PEG_conv(MessagePassing):\n",
    "    \n",
    "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
    "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        feats_dim (int): Size of node features.\n",
    "        pos_dim (int): Size of positional encoding.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
    "            cached version for further executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        normalize (bool, optional): Whether to add self-loops and compute\n",
    "            symmetric normalization coefficients on the fly.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        update_coors (bool): Whether to update positional encodings.\n",
    "        use_formerinfo (bool): Whether to use previous layer's output to update node features.\n",
    "        norm_coors (bool): Whether to normalize positional encodings. Only used when update_coors = True. \n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_feats_dim: int, pos_dim: int, out_feats_dim: int, edge_mlp_dim: int = 32,\n",
    "                 improved: bool = False, cached: bool = False,\n",
    "                 add_self_loops: bool = True, normalize: bool = True,\n",
    "                 bias: bool = True, update_coors: bool = False,\n",
    "                 use_formerinfo: bool = False, norm_coors = True, **kwargs):\n",
    "\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super(PEG_conv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_feats_dim = in_feats_dim\n",
    "        self.out_feats_dim = out_feats_dim\n",
    "        self.pos_dim = pos_dim\n",
    "        self.update_coors = update_coors\n",
    "        self.use_formerinfo = use_formerinfo\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "        self.edge_mlp_dim = edge_mlp_dim\n",
    "        self.coors_norm = CoorsNorm() if norm_coors else nn.Identity()\n",
    "\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "        \n",
    "        self.edge_mlp1 = nn.Linear(1, edge_mlp_dim)\n",
    "        self.edge_mlp2 = nn.Linear(edge_mlp_dim, 1)\n",
    "        self.weight_withformer = Parameter(torch.Tensor(in_feats_dim + in_feats_dim, out_feats_dim))\n",
    "        self.weight_noformer = Parameter(torch.Tensor(in_feats_dim, out_feats_dim))\n",
    "        self.coors_mlp = nn.Sequential(\n",
    "            nn.Linear(pos_dim, pos_dim * 4),\n",
    "            nn.Linear(pos_dim * 4, 1)\n",
    "        ) if update_coors else None\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_feats_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight_withformer)\n",
    "        glorot(self.weight_noformer)\n",
    "        zeros(self.bias)\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "\n",
    "    def forward(self,x: Tensor, edge_index: Adj, \n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        coors, feats = x[:, :self.pos_dim], x[:, self.pos_dim:]\n",
    "        \n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, feats.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, feats.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "        else:\n",
    "            print('We normalize the adjacent matrix in PEG.')\n",
    "        \n",
    "        \n",
    "        rel_coors = coors[edge_index[0]] - coors[edge_index[1]]\n",
    "        neighbour_coors = coors[edge_index[1]]\n",
    "        rel_dist  = (rel_coors ** 2).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        # pos: l2 norms\n",
    "        # rel_coors: used in updating positional encodings, not required for PEG\n",
    "        hidden_out, coors_out = self.propagate(edge_index, x = feats, edge_weight=edge_weight, pos=rel_dist, coors=coors, rel_coors=rel_coors, neighbour_coors = neighbour_coors,\n",
    "                             size=None)\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.bias is not None:\n",
    "            hidden_out += self.bias\n",
    "\n",
    "        return torch.cat([coors_out, hidden_out], dim=-1)\n",
    "\n",
    "\n",
    "    def message(self, x_i: Tensor, x_j: Tensor, edge_weight: OptTensor, pos) -> Tensor:\n",
    "        PE_edge_weight = self.edge_mlp1(pos)\n",
    "        PE_edge_weight = self.edge_mlp2(PE_edge_weight)\n",
    "        PE_edge_weight = torch.sigmoid(PE_edge_weight)\n",
    "        return x_j if edge_weight is None else PE_edge_weight * edge_weight.view(-1, 1) * x_j\n",
    "    \n",
    "    def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n",
    "        \"\"\"The initial call to start propagating messages.\n",
    "            Args:\n",
    "            `edge_index` holds the indices of a general (sparse)\n",
    "                assignment matrix of shape :obj:`[N, M]`.\n",
    "            size (tuple, optional) if none, the size will be inferred\n",
    "                and assumed to be quadratic.\n",
    "            **kwargs: Any additional data which is needed to construct and\n",
    "                aggregate messages, and to update node embeddings.\n",
    "        \"\"\"\n",
    "        size = self.__check_input__(edge_index, size)\n",
    "        coll_dict = self.__collect__(self.__user_args__,\n",
    "                                     edge_index, size, kwargs)\n",
    "        msg_kwargs = self.inspector.distribute('message', coll_dict)\n",
    "        aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n",
    "        update_kwargs = self.inspector.distribute('update', coll_dict)\n",
    "\n",
    "        # get messages\n",
    "        m_ij = self.message(**msg_kwargs)\n",
    "\n",
    "\n",
    "        m_i = self.aggregate(m_ij, **aggr_kwargs)\n",
    "        \n",
    "        # update coors if specified\n",
    "        if self.update_coors:\n",
    "            coor_wij = self.coors_mlp(m_ij)\n",
    "            kwargs[\"neighbour_coors\"] = self.coors_norm(kwargs[\"neighbour_coors\"])\n",
    "            mhat_i = self.aggregate(coor_wij * kwargs[\"neighbour_coors\"], **aggr_kwargs)\n",
    "            coors_out = kwargs[\"coors\"] + mhat_i\n",
    "        else:\n",
    "            coors_out = kwargs[\"coors\"]\n",
    "        \n",
    "        \n",
    "        hidden_feats = kwargs[\"x\"]\n",
    "        if self.use_formerinfo:\n",
    "            hidden_out = torch.cat([hidden_feats, m_i], dim = -1)\n",
    "            hidden_out = hidden_out @ self.weight_withformer\n",
    "        else:\n",
    "            hidden_out = m_i\n",
    "            hidden_out = hidden_out @ self.weight_noformer\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # return tuple\n",
    "        return self.update((hidden_out, coors_out), **update_kwargs)\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a94471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from PEG-PYG import PEG_conv\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d7553",
   "metadata": {},
   "source": [
    "# Link prediction example for PEG (cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7688549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'cuda:{7}' if torch.cuda.is_available() else 'cpu'\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "daf1b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Cora'\n",
    "path = osp.join('.', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "print(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2da1343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(test_neg_edge_index=[2, 527], test_pos_edge_index=[2, 527], train_neg_adj_mask=[2708, 2708], train_pos_edge_index=[2, 8976], val_neg_edge_index=[2, 263], val_pos_edge_index=[2, 263], x=[2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8130a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build train matrix for PE preparation\n",
    "import copy\n",
    "train_graph = copy.deepcopy(dataset[0])\n",
    "train_graph.edge_index = data.train_pos_edge_index\n",
    "G = to_networkx(train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d789e04",
   "metadata": {},
   "source": [
    "# We use Deepwalk to calculate PE in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a1e45605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "54e92f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_num(num, workers):\n",
    "    if num % workers == 0:\n",
    "        return [num//workers]*workers\n",
    "    else:\n",
    "        return [num//workers]*workers + [num % workers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c76682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified from \n",
    "class RandomWalker:\n",
    "    def __init__(self, G, p=1, q=1, use_rejection_sampling=0):\n",
    "        \"\"\"\n",
    "        :param G:\n",
    "        :param p: Return parameter,controls the likelihood of immediately revisiting a node in the walk.\n",
    "        :param q: In-out parameter,allows the search to differentiate between “inward” and “outward” nodes\n",
    "        :param use_rejection_sampling: Whether to use the rejection sampling strategy in node2vec.\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.use_rejection_sampling = use_rejection_sampling\n",
    "    \n",
    "    def deepwalk_walk(self, walk_length, start_node):\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = list(self.G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                walk.append(random.choice(cur_nbrs))\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, workers=1, verbose=0):\n",
    "\n",
    "        G = self.G\n",
    "\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
    "            delayed(self._simulate_walks)(nodes, num, walk_length) for num in\n",
    "            partition_num(num_walks, workers))\n",
    "\n",
    "        walks = list(itertools.chain(*results))\n",
    "\n",
    "        return walks\n",
    "    \n",
    "    def _simulate_walks(self, nodes, num_walks, walk_length,):\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for v in nodes:\n",
    "                if self.p == 1 and self.q == 1:\n",
    "                    walks.append(self.deepwalk_walk(\n",
    "                        walk_length=walk_length, start_node=v))\n",
    "                else:\n",
    "                    return (\"only work for DeepWalk\")\n",
    "        return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21177eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "class DeepWalk:\n",
    "    def __init__(self, graph, walk_length = 80, num_walks = 10, workers=1):\n",
    "\n",
    "        self.graph = graph\n",
    "        self.w2v_model = None\n",
    "        self._embeddings = {}\n",
    "\n",
    "        self.walker = RandomWalker(\n",
    "            graph, p=1, q=1, )\n",
    "        self.sentences = self.walker.simulate_walks(\n",
    "            num_walks=num_walks, walk_length=walk_length, workers=workers, verbose=1)\n",
    "\n",
    "    def train(self, embed_size=128, window_size=5, workers=3, iter=3, **kwargs):\n",
    "\n",
    "        kwargs[\"sentences\"] = self.sentences\n",
    "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "        kwargs[\"vector_size\"] = embed_size\n",
    "        kwargs[\"sg\"] = 1  # skip gram\n",
    "        kwargs[\"hs\"] = 1  # deepwalk use Hierarchical Softmax\n",
    "        kwargs[\"workers\"] = workers\n",
    "        kwargs[\"window\"] = window_size\n",
    "        kwargs[\"epochs\"] = iter\n",
    "\n",
    "        print(\"Learning embedding vectors...\")\n",
    "        model = Word2Vec(**kwargs)\n",
    "        print(\"Learning embedding vectors done!\")\n",
    "\n",
    "        self.w2v_model = model\n",
    "        return model\n",
    "\n",
    "    def get_embeddings(self,):\n",
    "        if self.w2v_model is None:\n",
    "            print(\"model not train\")\n",
    "            return {}\n",
    "\n",
    "        self._embeddings = {}\n",
    "        for word in self.graph.nodes():\n",
    "            self._embeddings[word] = self.w2v_model.wv[word]\n",
    "\n",
    "        return self._embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ed91ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n"
     ]
    }
   ],
   "source": [
    "model_emb = DeepWalk(G,walk_length=80, num_walks=10,workers=1)#init model\n",
    "model_emb.train(embed_size = 128)# train model\n",
    "emb = model_emb.get_embeddings()# get embedding vectors\n",
    "embeddings = []\n",
    "for i in range(len(emb)):\n",
    "    embeddings.append(emb[i])\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6efbbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_feats_dim, pos_dim, hidden_dim, use_former_information = False, update_coors = False):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.in_feats_dim = in_feats_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pos_dim = pos_dim\n",
    "        self.use_former_information = use_former_information\n",
    "        self.update_coors = update_coors\n",
    "        \n",
    "        self.conv1 = PEG_conv(in_feats_dim = in_feats_dim, pos_dim = pos_dim, out_feats_dim = hidden_dim,\n",
    "                               use_formerinfo = use_former_information, update_coors = update_coors)\n",
    "        self.conv2 = PEG_conv(in_feats_dim = hidden_dim, pos_dim = pos_dim, out_feats_dim = hidden_dim,\n",
    "                               use_formerinfo = use_former_information, update_coors = update_coors)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        x = self.conv1(x, pos_edge_index)\n",
    "        x = self.conv2(x, pos_edge_index)\n",
    "        pos_dim = self.pos_dim\n",
    "        \n",
    "        nodes_first = x[ : , pos_dim: ][edge_index[0]]\n",
    "        nodes_second = x[ : , pos_dim: ][edge_index[1]]\n",
    "        pos_first = x[ : , :pos_dim ][edge_index[0]]\n",
    "        pos_second = x[ : , :pos_dim ][edge_index[1]]\n",
    "        \n",
    "        positional_encoding = ((pos_first - pos_second)**2).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        pred = (nodes_first * nodes_second).sum(dim=-1)  # dot product \n",
    "        out = self.fc(torch.cat([pred.reshape(len(pred), 1),positional_encoding.reshape(len(positional_encoding), 1)], 1))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def loss(self, pred, link_label):\n",
    "        return self.loss_fn(pred, link_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a153e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f1bea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d158c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat((torch.tensor(embeddings), node_features), 1)\n",
    "x = x.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e9b0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    link_logits = model(x, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    link_logits = link_logits.reshape(len(link_logits),)\n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        model.fc.weight[0][0].clamp_(1e-5,100)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        link_logits = model(x, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        \n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9112a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(in_feats_dim = 1433, pos_dim = 128, hidden_dim = 128,\n",
    "            use_former_information = False, update_coors = False)\n",
    "    \n",
    "data = data.to(device)    \n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001, weight_decay= 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8882dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.1034, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 020, Loss: 0.1062, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 030, Loss: 0.0999, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 040, Loss: 0.0947, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 050, Loss: 0.0875, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 060, Loss: 0.0812, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 070, Loss: 0.0836, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 080, Loss: 0.0758, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 090, Loss: 0.0769, Val: 0.8961, Test: 0.9276\n",
      "Epoch: 100, Loss: 0.0740, Val: 0.8961, Test: 0.9276\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 10 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0b526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
